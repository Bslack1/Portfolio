set.seed(222)
wordcloud(words = names(wordFreq),
freq = wordFreq)
#read file again
SWaterData <- read.csv(file.choose(), header = T)
Watertweets <- iconv(SWaterData$Tweet, to = "UTF-8")
#obtain emotions sentiment scores
sentscores <- get_nrc_sentiment(tweets)
#obtain emotions sentiment scores
sentscores <- get_nrc_sentiment(Watertweets)
head(sentscores)
#to look at a specific tweet
tweets[4]
#to look at a specific tweet
tweets[4]
#to look at a specific tweet
Watertweets[4]
#find specific words
get_nrc_sentiment('no')
barplot(colSums(sentscores),
las = 2,
col = rainbow(10),
ylab = "count",
main = "Sentiment Scores VarNAme")
#afinn word sentiment
get_sentiment(
Watertweets,
method = "afinn",
path_to_tagger = NULL,
cl = NULL,
language = "english",
lexicon = NULL,
regex = "[^A-Za-z']+",
lowercase = TRUE
)
WaterSent <- get_sentiment('afinn', Watertweets)
summary(Watertweets)
#afinn word sentiment
WaterSent <- get_sentiment(
Watertweets,
method = "afinn",
path_to_tagger = NULL,
cl = NULL,
language = "english",
lexicon = NULL,
regex = "[^A-Za-z']+",
lowercase = TRUE)
summary(WaterSent)
write_xlsx(WaterSent, "C:\\Users\\Administrator\\Documents\\Data Sets\\Water\\Excel Data\\WaterSent.xlsx")
SentDataWater <- data.frame(WaterSent)
View(SentDataWater)
write_xlsx(SentDataWater, "C:\\Users\\Administrator\\Documents\\Data Sets\\Water\\Excel Data\\WaterSent.xlsx")
library(tm)
library(writexl)
#load file, read it into a variable so it can be cleaned and structured
cableData <- read.csv(file.choose(), header = T)
#load file, read it into a variable so it can be cleaned and structured
cableData <- read.csv(file.choose(), header = T)
#shows all the columns in file to pick the right col for tweets
str(cableData)
#make each tweet viewed as a document using corpus, verify
cableTweetList <- iconv(cableData$Tweet)
cableTweetList <- Corpus(VectorSource(cableTweetList))
inspect(cableTweetList[1:5])
#clean the text
cableTweetList <- tm_map(cableTweetList, tolower)
#function to specify removal of punct without removing #
removeMostPunctuation<-
function (x, preserve_intra_word_dashes = True)
{
rmpunct <- function(x) {
x <- gsub("#", "\002", x)
x <- gsub("[[:punct:]]+", "", x)
gsub("\002", "#", x, fixed = TRUE)
}
if (preserve_intra_word_dashes) {
x <- gsub("(\\w)-(\\w)", "\\1\001\\2", x)
x <- rmpunct(x)
gsub("\001", "-", x, fixed = TRUE)
} else {
rmpunct(x)
}
}
cableTweetList <- tm_map(cableTweetList, content_transformer(removeMostPunctuation),
preserve_intra_word_dashes = TRUE)
cableTweetList <- tm_map(cableTweetList, removeNumbers)
CleanSet <- tm_map(waterTweetList, removeWords, stopwords('english'))
CleanSet <- tm_map(cableTweetList, removeWords, stopwords('english'))
#function to remove urls
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
CleanSet <- tm_map(CleanSet, content_transformer(removeURL))
#remove to names that appear in every tweet discovered with tdm, multiples separate with comma consider removing other freq words
CleanSet <- tm_map(CleanSet, removeWords, c('coxcomm', 'xfinity', 'googlefiber', 'suddenlink', 'mediacomsupport', 'getspectrum', 'frontiercorp', 'verizonfios', 'dish', 'optimum'))
#remove white space & verify
CleanSet <- tm_map(CleanSet, stripWhitespace)
inspect(CleanSet[1:5])
library(syuzhet)
library(wordcloud)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
#up memory limit
memory.limit(size=56000)
#run garbage collector to free up memory
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)
#term document matrix - puts unstructured to rows and columns for word frequency
tdm <- TermDocumentMatrix(CleanSet)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
#word frequency
wordFreq <- rowSums(tdm)
#afinn word sentiment
CableSent <- get_sentiment(
Watertweets,
method = "afinn",
path_to_tagger = NULL,
cl = NULL,
language = "english",
lexicon = NULL,
regex = "[^A-Za-z']+",
lowercase = TRUE)
#afinn word sentiment
CableSent <- get_sentiment(
cabletweets,
method = "afinn",
path_to_tagger = NULL,
cl = NULL,
language = "english",
lexicon = NULL,
regex = "[^A-Za-z']+",
lowercase = TRUE)
#read file again
SCableData <- read.csv(file.choose(), header = T)
Cabletweets <- iconv(SCableData$Tweet, to = "UTF-8")
#afinn word sentiment
CableSent <- get_sentiment(
Cabletweets,
method = "afinn",
path_to_tagger = NULL,
cl = NULL,
language = "english",
lexicon = NULL,
regex = "[^A-Za-z']+",
lowercase = TRUE)
summary(CableSent)
SentDataCable <- data.frame(CableSent)
write_xlsx(SentDataCable, "C:\\Users\\Administrator\\Documents\\Data Sets\\Cable\\Excel Files\\CableSent.xlsx")
#read file again
SElecData <- read.csv(file.choose(), header = T)
Electweets <- iconv(SElecData$Tweet, to = "UTF-8")
#afinn word sentiment
ElecSent <- get_sentiment(
Electweets,
method = "afinn",
path_to_tagger = NULL,
cl = NULL,
language = "english",
lexicon = NULL,
regex = "[^A-Za-z']+",
lowercase = TRUE)
summary(ElecSent)
write_xlsx(SentDataElec, "C:\\Users\\Administrator\\Documents\\Data Sets\\Cable\\Excel Data\\ElecSent.xlsx")
SentDataElec <- data.frame(ElecSent)
write_xlsx(SentDataElec, "C:\\Users\\Administrator\\Documents\\Data Sets\\Cable\\Excel Data\\ElecSent.xlsx")
write_xlsx(SentDataElec, "C:\\Users\\Administrator\\Documents\\Data Sets\\Electric\\Excel Data\\ElecSent.xlsx")
#read file again
SElecData <- read.csv(file.choose(), header = T)
Electweets <- iconv(SElecData$Tweet, to = "UTF-8")
#afinn word sentiment
ElecSent <- get_sentiment(
Electweets,
method = "afinn",
path_to_tagger = NULL,
cl = NULL,
language = "english",
lexicon = NULL,
regex = "[^A-Za-z']+",
lowercase = TRUE)
summary(ElecSent)
SentDataElec <- data.frame(ElecSent)
write_xlsx(SentDataElec, "C:\\Users\\Administrator\\Documents\\Data Sets\\Gas\\GasSent.xlsx")
#read file again
SElecData <- read.csv(file.choose(), header = T)
Electweets <- iconv(SElecData$Tweet, to = "UTF-8")
#afinn word sentiment
ElecSent <- get_sentiment(
Electweets,
method = "afinn",
path_to_tagger = NULL,
cl = NULL,
language = "english",
lexicon = NULL,
regex = "[^A-Za-z']+",
lowercase = TRUE)
summary(ElecSent)
SentDataElec <- data.frame(ElecSent)
write_xlsx(SentDataElec, "C:\\Users\\Administrator\\Documents\\Data Sets\\Multi Utility\\MultiSent.xlsx")
library(tm)
library(writexl)
#load file, read it into a variable so it can be cleaned and structured
cableData <- read.csv(file.choose(), header = T)
#load file, read it into a variable so it can be cleaned and structured
cableData <- read.csv(file.choose(), header = T)
#shows all the columns in file to pick the right col for tweets
str(cableData)
#make each tweet viewed as a document using corpus, verify
cableTweetList <- iconv(cableData$Tweet)
cableTweetList <- Corpus(VectorSource(cableTweetList))
inspect(cableTweetList[1:5])
#clean the text
cableTweetList <- tm_map(cableTweetList, tolower)
#function to specify removal of punct without removing #
removeMostPunctuation<-
function (x, preserve_intra_word_dashes = True)
{
rmpunct <- function(x) {
x <- gsub("#", "\002", x)
x <- gsub("[[:punct:]]+", "", x)
gsub("\002", "#", x, fixed = TRUE)
}
if (preserve_intra_word_dashes) {
x <- gsub("(\\w)-(\\w)", "\\1\001\\2", x)
x <- rmpunct(x)
gsub("\001", "-", x, fixed = TRUE)
} else {
rmpunct(x)
}
}
cableTweetList <- tm_map(cableTweetList, content_transformer(removeMostPunctuation),
preserve_intra_word_dashes = TRUE)
cableTweetList <- tm_map(cableTweetList, removeNumbers)
CleanSet <- tm_map(cableTweetList, removeWords, stopwords('english'))
#function to remove urls
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
CleanSet <- tm_map(CleanSet, content_transformer(removeURL))
#remove to names that appear in every tweet discovered with tdm, multiples separate with comma consider removing other freq words
CleanSet <- tm_map(CleanSet, removeWords, c('coxcomm', 'xfinity', 'googlefiber',
'suddenlink', 'mediacomsupport', 'getspectrum',
'frontiercorp', 'verizonfios', 'dish', 'optimum'))
CleanSet <- tm_map(CleanSet, removeWords, c('sce', 'southerncompany', 'aepnews', 'aepohio',
'nrgenergy', 'oncor', 'entergy',
'delmarvaconnect', 'acelecconnect', 'comed'))
CleanSet <- tm_map(CleanSet, removeWords, c('socalgas', 'atmosenergy'))
CleanSet <- tm_map(CleanSet, removeWords, c('itroninc', 'dukeenergy', 'pge4me', 'dte',
'constellation', 'mybge', 'pecoconnect',
'sempraenergy', 'centerpoint', 'psegdelivers',
'psegli', 'consumersenergy', 'eversourcecorp',
'eversourcect', 'tvanews', 'dominionenergy',
'sdge'))
CleanSet <- tm_map(CleanSet, removeWords, c('amwater', 'wvamwater', 'njamwater',
'paamwater', 'moamwater', 'vaamwater',
'nyamwater', 'inamwater', 'iaamwater',
'ilamwater', 'kyamwater', 'tnamwater',
'myaquaamerica', 'yorkwater', 'calwater',
'suezna','sjwaterco', 'goldenstateh2o',
'ctwater', 'middlesexwater'))
#remove white space & verify
CleanSet <- tm_map(CleanSet, stripWhitespace)
inspect(CleanSet[1:5])
library(syuzhet)
library(wordcloud)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
#up memory limit
memory.limit(size=56000)
#run garbage collector to free up memory
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)
#term document matrix - puts unstructured to rows and columns for word frequency
tdm <- TermDocumentMatrix(CleanSet)
tdm <- as.matrix(tdm)
#run garbage collector to free up memory
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)
tdm <- as.matrix(tdm)
#up memory limit
memory.limit(size=56000)
#run garbage collector to free up memory
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)
tdm <- as.matrix(tdm)
#run garbage collector to free up memory
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)
#up memory limit
memory.limit(size=56000)
tdm <- as.matrix(tdm)
memory.limit(size=120000)
tdm <- as.matrix(tdm)
#word frequency
wordFreq <- rowSums(tdm)
library(tm)
library(writexl)
#load file, read it into a variable so it can be cleaned and structured
cableData <- read.csv(file.choose(), header = T)
#shows all the columns in file to pick the right col for tweets
str(cableData)
#make each tweet viewed as a document using corpus, verify
cableTweetList <- iconv(cableData$Tweet)
cableTweetList <- Corpus(VectorSource(cableTweetList))
inspect(cableTweetList[1:5])
#clean the text
cableTweetList <- tm_map(cableTweetList, tolower)
#function to specify removal of punct without removing #
removeMostPunctuation<-
function (x, preserve_intra_word_dashes = True)
{
rmpunct <- function(x) {
x <- gsub("#", "\002", x)
x <- gsub("[[:punct:]]+", "", x)
gsub("\002", "#", x, fixed = TRUE)
}
if (preserve_intra_word_dashes) {
x <- gsub("(\\w)-(\\w)", "\\1\001\\2", x)
x <- rmpunct(x)
gsub("\001", "-", x, fixed = TRUE)
} else {
rmpunct(x)
}
}
cableTweetList <- tm_map(cableTweetList, content_transformer(removeMostPunctuation),
preserve_intra_word_dashes = TRUE)
cableTweetList <- tm_map(cableTweetList, removeNumbers)
CleanSet <- tm_map(cableTweetList, removeWords, stopwords('english'))
#function to remove urls
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
CleanSet <- tm_map(CleanSet, content_transformer(removeURL))
#remove to names that appear in every tweet discovered with tdm, multiples separate with comma consider removing other freq words
CleanSet <- tm_map(CleanSet, removeWords, c('coxcomm', 'xfinity', 'googlefiber',
'suddenlink', 'mediacomsupport',
'getspectrum', 'frontiercorp',
'verizonfios', 'dish', 'optimum'))
#remove white space & verify
CleanSet <- tm_map(CleanSet, stripWhitespace)
inspect(CleanSet[1:5])
library(syuzhet)
library(wordcloud)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
#up memory limit
memory.limit(size=56000)
#run garbage collector to free up memory
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)
#term document matrix - puts unstructured to rows and columns for word frequency
tdm <- TermDocumentMatrix(CleanSet)
#run garbage collector to free up memory
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
#word frequency
wordFreq <- rowSums(tdm)
#removes words appearing less than 50 times
wordFreq <- subset(wordFreq, wordFreq >= 50)
#word cloud see video for all options, wc2 can put into other words or shapes
wordFreq <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(wordFreq),
freq = wordFreq)
#word frequency
wordFreq <- rowSums(tdm)
tdm <- as.matrix(tdm)
#up memory limit
memory.limit(size=56000)
#run garbage collector to free up memory
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)
tdm <- as.matrix(tdm)
#run garbage collector to free up memory
gc(verbose = getOption("verbose"), reset = FALSE, full = TRUE)
#up memory limit
memory.limit(size=120000)
tdm <- as.matrix(tdm)
library(tm)
library(writexl)
#load file, read it into a variable so it can be cleaned and structured
cableData <- read.csv(file.choose(), header = T)
#load file, read it into a variable so it can be cleaned and structured
cableData <- read.csv(file.choose(), header = T)
#shows all the columns in file to pick the right col for tweets
str(cableData)
#make each tweet viewed as a document using corpus, verify
cableTweetList <- iconv(cableData$Tweet)
cableTweetList <- Corpus(VectorSource(cableTweetList))
inspect(cableTweetList[1:5])
#clean the text
cableTweetList <- tm_map(cableTweetList, tolower)
#function to specify removal of punct without removing #
removeMostPunctuation<-
function (x, preserve_intra_word_dashes = True)
{
rmpunct <- function(x) {
x <- gsub("#", "\002", x)
x <- gsub("[[:punct:]]+", "", x)
gsub("\002", "#", x, fixed = TRUE)
}
if (preserve_intra_word_dashes) {
x <- gsub("(\\w)-(\\w)", "\\1\001\\2", x)
x <- rmpunct(x)
gsub("\001", "-", x, fixed = TRUE)
} else {
rmpunct(x)
}
}
cableTweetList <- tm_map(cableTweetList, content_transformer(removeMostPunctuation),
preserve_intra_word_dashes = TRUE)
cableTweetList <- tm_map(cableTweetList, removeNumbers)
CleanSet <- tm_map(cableTweetList, removeWords, stopwords('english'))
#function to remove urls
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
CleanSet <- tm_map(CleanSet, content_transformer(removeURL))
#remove to names that appear in every tweet discovered with tdm, multiples separate with comma consider removing other freq words
CleanSet <- tm_map(CleanSet, removeWords, c('coxcomm', 'xfinity', 'googlefiber',
'suddenlink', 'mediacomsupport',
'getspectrum', 'frontiercorp',
'verizonfios', 'dish', 'optimum'))
#remove white space & verify
CleanSet <- tm_map(CleanSet, stripWhitespace)
inspect(CleanSet[1:5])
library(syuzhet)
library(wordcloud)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
#up memory limit
memory.limit(size=120000)
#term document matrix - puts unstructured to rows and columns for word frequency
tdm <- TermDocumentMatrix(CleanSet)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
#word frequency
wordFreq <- rowSums(tdm)
#removes words appearing less than 50 times
wordFreq <- subset(wordFreq, wordFreq >= 50)
#word cloud see video for all options, wc2 can put into other words or shapes
wordFreq <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(wordFreq),
freq = wordFreq)
library(tm)
library(writexl)
#load file, read it into a variable so it can be cleaned and structured
cableData <- read.csv(file.choose(), header = T)
#shows all the columns in file to pick the right col for tweets
str(cableData)
#make each tweet viewed as a document using corpus, verify
cableTweetList <- iconv(cableData$Tweet)
cableTweetList <- Corpus(VectorSource(cableTweetList))
inspect(cableTweetList[1:5])
#clean the text
cableTweetList <- tm_map(cableTweetList, tolower)
#function to specify removal of punct without removing #
removeMostPunctuation<-
function (x, preserve_intra_word_dashes = True)
{
rmpunct <- function(x) {
x <- gsub("#", "\002", x)
x <- gsub("[[:punct:]]+", "", x)
gsub("\002", "#", x, fixed = TRUE)
}
if (preserve_intra_word_dashes) {
x <- gsub("(\\w)-(\\w)", "\\1\001\\2", x)
x <- rmpunct(x)
gsub("\001", "-", x, fixed = TRUE)
} else {
rmpunct(x)
}
}
cableTweetList <- tm_map(cableTweetList, content_transformer(removeMostPunctuation),
preserve_intra_word_dashes = TRUE)
cableTweetList <- tm_map(cableTweetList, removeNumbers)
CleanSet <- tm_map(cableTweetList, removeWords, stopwords('english'))
#function to remove urls
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
CleanSet <- tm_map(CleanSet, content_transformer(removeURL))
#remove to names that appear in every tweet discovered with tdm, multiples separate with comma consider removing other freq words
CleanSet <- tm_map(CleanSet, removeWords, c('coxcomm', 'xfinity', 'googlefiber',
'suddenlink', 'mediacomsupport',
'getspectrum', 'frontiercorp',
'verizonfios', 'dish', 'optimum'))
#remove white space & verify
CleanSet <- tm_map(CleanSet, stripWhitespace)
inspect(CleanSet[1:5])
library(syuzhet)
library(wordcloud)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
#term document matrix - puts unstructured to rows and columns for word frequency
tdm <- TermDocumentMatrix(CleanSet)
tdm <- as.matrix(tdm)
#word frequency
wordFreq <- rowSums(tdm)
#removes words appearing less than 50 times
wordFreq <- subset(wordFreq, wordFreq >= 10)
#word cloud see video for all options, wc2 can put into other words or shapes
wordFreq <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(wordFreq), colors = 'rainbow',
freq = wordFreq)
wordcloud(words = names(wordFreq), colors = "rainbow",
freq = wordFreq)
wordcloud(words = names(wordFreq), colors,
freq = wordFreq)
wordcloud(words = names(wordFreq), colors = TRUE,
freq = wordFreq)
